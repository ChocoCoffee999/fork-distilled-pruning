{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee04858-9ce2-4b0f-a0cd-00d117251eb8",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf200b-4087-4836-a422-0630818bff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install kornia\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f51ecf34-d1ab-4285-beb2-0aaf3e5bdb60",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:00.691279Z",
     "iopub.status.busy": "2023-05-31T18:20:00.690678Z",
     "iopub.status.idle": "2023-05-31T18:20:11.792732Z",
     "shell.execute_reply": "2023-05-31T18:20:11.791912Z",
     "shell.execute_reply.started": "2023-05-31T18:20:00.691257Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (66.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: kornia in /usr/local/lib/python3.9/dist-packages (0.6.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from kornia) (23.0)\n",
      "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from kornia) (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.1->kornia) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: optuna in /usr/local/lib/python3.9/dist-packages (3.2.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.41)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.11.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.23.4)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.9/dist-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import resnet50, resnet34, resnet18, wide_resnet50_2, ResNet50_Weights, alexnet\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from flax.training import checkpoints\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#Import from MTT code\n",
    "from networks import ConvNet, AlexNet\n",
    "from distill import ParamDiffAug\n",
    "from utils import evaluate_synset, get_network\n",
    "import argparse\n",
    "\n",
    "import optuna\n",
    "import kornia\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a4af9-0b34-45f9-bf6f-85f3ae6cadb7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131a0bf1-a86b-4af0-9767-54de55893237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:11.796077Z",
     "iopub.status.busy": "2023-05-31T18:20:11.795927Z",
     "iopub.status.idle": "2023-05-31T18:20:11.871066Z",
     "shell.execute_reply": "2023-05-31T18:20:11.870340Z",
     "shell.execute_reply.started": "2023-05-31T18:20:11.796059Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load distilled data from MTT, if not in downloads directory, download from MTT repository: https://github.com/GeorgeCazenavette/mtt-distillation/\n",
    "labels_train = torch.load('./data/cifar100_50ipc_labels.pt')\n",
    "images_train = torch.load('./data/cifar100_50ipc_images.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe7dd09d-22f0-43c7-8a0d-7afddd0ab18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:11.872662Z",
     "iopub.status.busy": "2023-05-31T18:20:11.872466Z",
     "iopub.status.idle": "2023-05-31T18:20:15.857947Z",
     "shell.execute_reply": "2023-05-31T18:20:15.857380Z",
     "shell.execute_reply.started": "2023-05-31T18:20:11.872646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load in real training data from pytorch\n",
    "batch_size = 256\n",
    "train_dataset = torchvision.datasets.CIFAR100(root = './data',\n",
    "                                                    train = True,\n",
    "                                                    transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),]),\n",
    "                                                    download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(root = './data',\n",
    "                                                    train = False,\n",
    "                                                    transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),]),\n",
    "                                                    download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594dc2db-0fd3-4a18-904d-94aa89e8e5c7",
   "metadata": {},
   "source": [
    "# Methods used for Distilled Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "893abb26-d2bb-406a-ab58-9d68cf9c6438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:15.859459Z",
     "iopub.status.busy": "2023-05-31T18:20:15.859097Z",
     "iopub.status.idle": "2023-05-31T18:20:15.884642Z",
     "shell.execute_reply": "2023-05-31T18:20:15.884226Z",
     "shell.execute_reply.started": "2023-05-31T18:20:15.859437Z"
    }
   },
   "outputs": [],
   "source": [
    "#Standard train function with hyperparameters used in paper set as default\n",
    "def train(model,train_loader, num_epochs, lr = .0008, weight_decay = .0008, gamma = .15, milestones = [50,65,80]):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma= gamma)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = cost(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "    pass\n",
    "\n",
    "#Standard test function, prints & returns test accuracy\n",
    "def test(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader): \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        accuracy = correct / total\n",
    "\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "#Helper function for prunable all pruning modules to work pytorch global pruning. \n",
    "#See global pruning section of this: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
    "def get_parameters_to_prune(model):\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    return tuple(parameters_to_prune)\n",
    "\n",
    "#Returns number of zeros and total number of prunable parameters of a model. Global Sparsity measured as: zero / total\n",
    "def sparsity_print(model):\n",
    "    prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=0)\n",
    "    zero = total = 0\n",
    "    for module, _ in get_parameters_to_prune(model):\n",
    "        zero += float(torch.sum(module.weight == 0))\n",
    "        total += float(module.weight.nelement())\n",
    "    print('Number of Zero Weights:', zero)\n",
    "    print('Total Number of Weights:', total)\n",
    "    print('Sparsity', zero/total)\n",
    "    #TODO: Implement Node Sparsity\n",
    "    return zero, total\n",
    "\n",
    "#Standard IMP with Weight Rewinding to the kth epoch in training, \n",
    "#name: a string that allows us to save models/logs appropriately, \n",
    "#path: the location of folder we save to,\n",
    "#start_iter: should normally be 0 but if a experiment stops halfway through it allows us to begin there,\n",
    "#amount = % params pruned each pruning iteration, \n",
    "#save_model: boolean to decide if we download the model at every iter, \n",
    "#reinit: is boolean value if we want to test results on reinitialized weights\n",
    "#reinit_model: is the specific model that holds the reinitialized weights, must be same type as model\n",
    "#seed: seed for pruning sequence, SEED IS NOT FOR MODEL INITIALIZATION FROM PAPER. We instead set seed outside the function for that.\n",
    "#num_epochs: number of epochs used for training\n",
    "def LotteryTicketRewinding(model, name, path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, k = 1, amount = .2, save_model = True, seed = 0, reinit = False, reinit_model = None):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = [] #keeps track of zeros at each iteration\n",
    "    totals = [] #keeps track of total parameters\n",
    "    acc = [] #keeps track of model accuracy at each pruning iteration\n",
    "    reinit_acc = [] #same as above but for reinitialized model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    #Create Rewind Weights after training K epochs\n",
    "    train(model, train_loader,num_epochs = k)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    #Finish off the pretraining\n",
    "    train(model, train_loader,num_epochs = num_epochs - k)\n",
    "\n",
    "    #Lottery Ticket Rewinding: Prune, Rewind, Train\n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('LTR Iteration:', i+1)\n",
    "        #Prune\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "        #Train Weights\n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        \n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        #Reinitialize the weights, train and validate on those new weights\n",
    "        if reinit:\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals,reinit_acc]))\n",
    "    \n",
    "    pass\n",
    "  \n",
    "#Generate full sparsity curve with retraining for random pruning, this is not a method to compute a single, high-sparsity model with random pruning. \n",
    "#This generates and trains models at all sparsities for comparison. \n",
    "#If you want to perform a single random pruning, then just use: prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.RandomUnstructured,amount=amount)\n",
    "def RandomPruning(model, name, path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, amount = .2, save_model = True, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    acc = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('Random Pruning Iteration:', i+1)\n",
    "        #Prune\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.RandomUnstructured,amount=amount)\n",
    "        #Train Weights\n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        \n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "\n",
    "            \n",
    "        #Rewind Weights to save them\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "                \n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals]))\n",
    "    \n",
    "#Distilled Pruning \n",
    "#See LotteryTicketRewinding for parameter description. \n",
    "#num_epochs_distilled: number of epochs used for distilled training\n",
    "#distilled_lr = learning rate for distilled training\n",
    "#validate = Retrains pruned model at each iteration on real data. This is used to generate a full sparsity curve, but is not efficient for finding a single distilled mask.\n",
    "#use validate false, and set end_iter to desired final iteration of the mask. Remember sparsity = 1-(1-amount)^end_iter\n",
    "def DistilledPruning(model, name, path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs_distilled = 1000, num_epochs_real = 60, k = 0, amount = .2, save_model = True, validate = False, seed = 0, reinit = False, reinit_model = None, distilled_lr = .01):\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    accs = []\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    reinit_acc = []\n",
    "    \n",
    "    #Create rewind weights at initailization\n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    \n",
    "    #Use if you want to try rewinding to an early point in training, this does not work well, so we suggest k=0 always.\n",
    "    if k != 0:\n",
    "        args = argparse.Namespace(lr_net=str(distilled_lr), device='cuda', epoch_eval_train=str(k),batch_train=512,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "        model_rewind, acc_train_list, acc_test = evaluate_synset(0, model_rewind,images_train,labels_train,test_loader,args)\n",
    "        \n",
    "    \n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('Distilled Pruning Iteration ', i)\n",
    "        #Set distilled pruning training args for MTT eval\n",
    "        args = argparse.Namespace(lr_net='.01', device='cuda', epoch_eval_train=str(num_epochs_distilled),batch_train=512,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "        #MTT Training on Distilled Data\n",
    "        model, acc_train_list, acc_test = evaluate_synset(i+1, model,images_train,labels_train,test_loader,args)\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "    \n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        #Rewind weights back to initialization and train on real data to validate this sparsity mask\n",
    "        if validate:\n",
    "            train(model, train_loader,num_epochs = num_epochs_real)\n",
    "            accs.append(test(model, test_loader))\n",
    "            zero, total = sparsity_print(model)\n",
    "            zeros.append(zero)\n",
    "            totals.append(total)\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "                    \n",
    "            np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n",
    "        \n",
    "        if reinit:\n",
    "            #Rewind Weights to Reinit Model\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs_real)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "            np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "    #If validate = False, then we still want to validate the final sparsity mask. just not all the masks.\n",
    "    if not validate:\n",
    "        train(model, train_loader,num_epochs = num_epochs_real)\n",
    "        acc = (test(model, test_loader))\n",
    "        zero, total = sparsity_print(model)\n",
    "        np.save(path + name + '_log', np.array([acc, zero, total, reinit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57203403-e2aa-4eae-a40e-3f40172945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save logs and models here\n",
    "path = './model_results_cifar100/' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f51b0f-23af-4196-a0e7-61cf69325e3b",
   "metadata": {},
   "source": [
    "For CIFAR 100, Distilled Pruning Hyperparameters = .085 distilled_lr and 1300 num_epochs_distilled. All else is in code and the same across experiments/datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e3c70-0bda-4973-8f31-579b6faea4c4",
   "metadata": {},
   "source": [
    "# Example Distilled Training Snippet for MTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e7c885a-47f1-49b3-8cff-97b6a057898e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T16:29:40.688940Z",
     "iopub.status.busy": "2023-05-30T16:29:40.688301Z",
     "iopub.status.idle": "2023-05-30T16:31:14.136578Z",
     "shell.execute_reply": "2023-05-30T16:31:14.135932Z",
     "shell.execute_reply.started": "2023-05-30T16:29:40.688914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1301/1301 [01:33<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 16:31:14] Evaluate_01: epoch = 1300 train time = 93 s train loss = 0.005592 train acc = 1.0000, test acc = 0.3993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_network('ConvNetW128', 3, 100)\n",
    "args = argparse.Namespace(lr_net='.09', device='cuda', epoch_eval_train=str(1300),batch_train=512,dataset='cifar100',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "model, acc_train_list, acc_test = evaluate_synset(1, model,images_train,labels_train,test_loader,args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c4635-85bd-4b61-b0a5-475f975b25fa",
   "metadata": {},
   "source": [
    "# Example Distilled Pruning for computing a single sparsity mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a9c03-806d-41df-8733-5b83c6f092fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = get_network('ConvNetW128', 3, 100)\n",
    "DistilledPruning(model, 'DistilledPruning_CIFAR100_ipc50_seed0_iter5', path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 5, num_epochs_distilled = 1300, num_epochs_real = 120, k = 0, amount = .2, save_model = False, validate = True, seed = 0, reinit = False, reinit_model = None, distilled_lr = .09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2d06e-5797-4f6d-a76c-5505413784d5",
   "metadata": {},
   "source": [
    "# Distilled Pruning Experiment for generating plots / validating all sparsity mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9478a-58a5-4e26-9289-90a36529db4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:33.240209Z",
     "iopub.status.busy": "2023-05-31T18:20:33.239478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Pruning Iteration  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1251 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 1251/1251 [00:55<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-31 18:21:30] Evaluate_01: epoch = 1250 train time = 55 s train loss = 0.029523 train acc = 1.0000, test acc = 0.3745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98/2071708352.py:197: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5176\n",
      "Number of Zero Weights: 100634.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.2000007949631137\n",
      "Distilled Pruning Iteration  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:54<00:00, 22.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-31 18:36:19] Evaluate_02: epoch = 1250 train time = 54 s train loss = 0.017830 train acc = 1.0000, test acc = 0.3730\n",
      "Test Accuracy: 0.5025\n",
      "Number of Zero Weights: 181141.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.3600010334520478\n",
      "Distilled Pruning Iteration  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:54<00:00, 22.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-31 18:51:10] Evaluate_03: epoch = 1250 train time = 54 s train loss = 0.004405 train acc = 1.0000, test acc = 0.3734\n",
      "Test Accuracy: 0.495\n",
      "Number of Zero Weights: 245546.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.4880000317985245\n",
      "Distilled Pruning Iteration  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:53<00:00, 23.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-31 19:06:01] Evaluate_04: epoch = 1250 train time = 53 s train loss = 0.003360 train acc = 1.0000, test acc = 0.3712\n",
      "Test Accuracy: 0.4791\n",
      "Number of Zero Weights: 297070.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.590399230475706\n",
      "Distilled Pruning Iteration  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:54<00:00, 22.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-31 19:20:54] Evaluate_05: epoch = 1250 train time = 54 s train loss = 0.003594 train acc = 1.0000, test acc = 0.3687\n",
      "Test Accuracy: 0.4738\n",
      "Number of Zero Weights: 338290.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.6723201793436785\n",
      "Distilled Pruning Iteration  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1251/1251 [00:54<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-31 19:35:49] Evaluate_06: epoch = 1250 train time = 54 s train loss = 0.014486 train acc = 1.0000, test acc = 0.3672\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3, 100)\n",
    "    DistilledPruning(model, 'DistilledPruning_CIFAR100_ipc50_seed' + str(i), path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 20, num_epochs_distilled = 1300, num_epochs_real = 120, k = 0, amount = .2, save_model = False, validate = True, seed = 0, reinit = False, reinit_model = None, distilled_lr = .09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7dd5f-18cc-4691-a55e-e6b889bfbad3",
   "metadata": {},
   "source": [
    "# IMP Pruning Experiment for generating plots / validating all sparsity mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1763be-3343-44f1-b2ad-3897c4c1adfe",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-26T02:40:42.664587Z",
     "iopub.status.busy": "2023-05-26T02:40:42.664303Z",
     "iopub.status.idle": "2023-05-26T06:26:45.241275Z",
     "shell.execute_reply": "2023-05-26T06:26:45.240325Z",
     "shell.execute_reply.started": "2023-05-26T02:40:42.664566Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTR Iteration: 1\n",
      "Number of Zero Weights: 63770.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.20000125451625853\n",
      "Test Accuracy: 0.8143\n",
      "LTR Iteration: 2\n",
      "Number of Zero Weights: 114786.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.36000225812926534\n",
      "Test Accuracy: 0.8132\n",
      "LTR Iteration: 3\n",
      "Number of Zero Weights: 155598.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.48800055198715375\n",
      "Test Accuracy: 0.8118\n",
      "LTR Iteration: 4\n",
      "Number of Zero Weights: 188248.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.590400441589723\n",
      "Test Accuracy: 0.8094\n",
      "LTR Iteration: 5\n",
      "Number of Zero Weights: 214368.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.6723203532717784\n",
      "Test Accuracy: 0.8101\n",
      "LTR Iteration: 6\n",
      "Number of Zero Weights: 235264.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.7378562826174228\n",
      "Test Accuracy: 0.806\n",
      "LTR Iteration: 7\n",
      "Number of Zero Weights: 251981.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.7902856533520675\n",
      "Test Accuracy: 0.8042\n",
      "LTR Iteration: 8\n",
      "Number of Zero Weights: 265354.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.8322272681653954\n",
      "Test Accuracy: 0.8029\n",
      "LTR Iteration: 9\n",
      "Number of Zero Weights: 276053.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.8657824417904456\n",
      "Test Accuracy: 0.7962\n",
      "LTR Iteration: 10\n",
      "Number of Zero Weights: 284612.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.8926259534323565\n",
      "Test Accuracy: 0.7945\n",
      "LTR Iteration: 11\n",
      "Number of Zero Weights: 291459.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9141001354877559\n",
      "Test Accuracy: 0.7851\n",
      "LTR Iteration: 12\n",
      "Number of Zero Weights: 296937.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.931280735648334\n",
      "Test Accuracy: 0.7749\n",
      "LTR Iteration: 13\n",
      "Number of Zero Weights: 301319.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9450239612605379\n",
      "Test Accuracy: 0.7725\n",
      "LTR Iteration: 14\n",
      "Number of Zero Weights: 304825.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9560197962665596\n",
      "Test Accuracy: 0.7663\n",
      "LTR Iteration: 15\n",
      "Number of Zero Weights: 307630.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9648170915295062\n",
      "Test Accuracy: 0.7679\n",
      "LTR Iteration: 16\n",
      "Number of Zero Weights: 309874.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9718549277398635\n",
      "Test Accuracy: 0.757\n",
      "LTR Iteration: 17\n",
      "Number of Zero Weights: 311669.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.97748456945002\n",
      "Test Accuracy: 0.751\n",
      "LTR Iteration: 18\n",
      "Number of Zero Weights: 313105.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9819882828181453\n",
      "Test Accuracy: 0.7341\n",
      "LTR Iteration: 19\n",
      "Number of Zero Weights: 314254.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9855918807707748\n",
      "Test Accuracy: 0.7274\n",
      "LTR Iteration: 20\n",
      "Number of Zero Weights: 315173.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9884741318747491\n",
      "Test Accuracy: 0.7071\n",
      "LTR Iteration: 21\n",
      "Number of Zero Weights: 315908.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9907793054997993\n",
      "Test Accuracy: 0.6792\n",
      "LTR Iteration: 22\n",
      "Number of Zero Weights: 316496.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9926234443998394\n",
      "Test Accuracy: 0.6619\n",
      "LTR Iteration: 23\n",
      "Number of Zero Weights: 316966.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.994097501003613\n",
      "Test Accuracy: 0.6368\n",
      "LTR Iteration: 24\n",
      "Number of Zero Weights: 317342.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9952767462866319\n",
      "Test Accuracy: 0.628\n",
      "LTR Iteration: 25\n",
      "Number of Zero Weights: 317643.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9962207697711762\n",
      "Test Accuracy: 0.5611\n",
      "LTR Iteration: 26\n",
      "Number of Zero Weights: 317884.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.996976615816941\n",
      "Test Accuracy: 0.5451\n",
      "LTR Iteration: 27\n",
      "Number of Zero Weights: 318077.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9975819199116821\n",
      "Test Accuracy: 0.5434\n",
      "LTR Iteration: 28\n",
      "Number of Zero Weights: 318231.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9980649086712163\n",
      "Test Accuracy: 0.4777\n",
      "LTR Iteration: 29\n",
      "Number of Zero Weights: 318354.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9984506724207146\n",
      "Test Accuracy: 0.461\n",
      "LTR Iteration: 30\n",
      "Number of Zero Weights: 318453.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9987611651947009\n",
      "Test Accuracy: 0.4082\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3,10)\n",
    "    LotteryTicketRewinding(model, 'IMP_CIFAR100_seed' + str(i), path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 120, k = 1, amount = .2, save_model = False, seed = i, reinit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188de44-d388-4a93-97b0-bd9c53f9f7f7",
   "metadata": {},
   "source": [
    "# Random Pruning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a75ef5b-b092-4069-ad01-997f9305da6a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-25T22:14:54.956970Z",
     "iopub.status.busy": "2023-05-25T22:14:54.956771Z",
     "iopub.status.idle": "2023-05-26T01:53:25.340736Z",
     "shell.execute_reply": "2023-05-26T01:53:25.340058Z",
     "shell.execute_reply.started": "2023-05-25T22:14:54.956949Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Pruning Iteration: 1\n",
      "Number of Zero Weights: 100634.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.2000007949631137\n",
      "Test Accuracy: 0.8126\n",
      "Random Pruning Iteration: 2\n",
      "Number of Zero Weights: 181141.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.3600010334520478\n",
      "Test Accuracy: 0.7985\n",
      "Random Pruning Iteration: 3\n",
      "Number of Zero Weights: 245547.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.4880020192063088\n",
      "Test Accuracy: 0.7913\n",
      "Random Pruning Iteration: 4\n",
      "Number of Zero Weights: 297070.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.590399230475706\n",
      "Test Accuracy: 0.781\n",
      "Random Pruning Iteration: 5\n",
      "Number of Zero Weights: 338290.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.6723201793436785\n",
      "Test Accuracy: 0.7751\n",
      "Random Pruning Iteration: 6\n",
      "Number of Zero Weights: 371266.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.7378569384380564\n",
      "Test Accuracy: 0.7669\n",
      "Random Pruning Iteration: 7\n",
      "Number of Zero Weights: 397646.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.7902847557873315\n",
      "Test Accuracy: 0.7684\n",
      "Random Pruning Iteration: 8\n",
      "Number of Zero Weights: 418750.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.8322270096667514\n",
      "Test Accuracy: 0.7458\n",
      "Random Pruning Iteration: 9\n",
      "Number of Zero Weights: 435634.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.8657824026965149\n",
      "Test Accuracy: 0.7493\n",
      "Random Pruning Iteration: 10\n",
      "Number of Zero Weights: 449141.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.8926263196387687\n",
      "Test Accuracy: 0.7298\n",
      "Random Pruning Iteration: 11\n",
      "Number of Zero Weights: 459946.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9141002607479013\n",
      "Test Accuracy: 0.727\n",
      "Random Pruning Iteration: 12\n",
      "Number of Zero Weights: 468590.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9312794136352073\n",
      "Test Accuracy: 0.7261\n",
      "Random Pruning Iteration: 13\n",
      "Number of Zero Weights: 475506.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9450243258712796\n",
      "Test Accuracy: 0.7153\n",
      "Random Pruning Iteration: 14\n",
      "Number of Zero Weights: 481038.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9560186657339099\n",
      "Test Accuracy: 0.6958\n",
      "Random Pruning Iteration: 15\n",
      "Number of Zero Weights: 485464.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9648149325871279\n",
      "Test Accuracy: 0.6827\n",
      "Random Pruning Iteration: 16\n",
      "Number of Zero Weights: 489005.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9718523435512593\n",
      "Test Accuracy: 0.6704\n",
      "Random Pruning Iteration: 17\n",
      "Number of Zero Weights: 491838.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9774826698041211\n",
      "Test Accuracy: 0.6413\n",
      "Random Pruning Iteration: 18\n",
      "Number of Zero Weights: 494104.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9819861358432969\n",
      "Test Accuracy: 0.6195\n",
      "Random Pruning Iteration: 19\n",
      "Number of Zero Weights: 495917.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9855893061561943\n",
      "Test Accuracy: 0.5957\n",
      "Random Pruning Iteration: 20\n",
      "Number of Zero Weights: 497367.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9884710474433986\n",
      "Test Accuracy: 0.5452\n",
      "Random Pruning Iteration: 21\n",
      "Number of Zero Weights: 498527.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.990776440473162\n",
      "Test Accuracy: 0.5109\n",
      "Random Pruning Iteration: 22\n",
      "Number of Zero Weights: 499455.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9926207548969728\n",
      "Test Accuracy: 0.4642\n",
      "Random Pruning Iteration: 23\n",
      "Number of Zero Weights: 500198.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9940973988806919\n",
      "Test Accuracy: 0.4061\n",
      "Random Pruning Iteration: 24\n",
      "Number of Zero Weights: 500792.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9952779191045535\n",
      "Test Accuracy: 0.3375\n",
      "Random Pruning Iteration: 25\n",
      "Number of Zero Weights: 501267.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.996221937802086\n",
      "Test Accuracy: 0.3026\n",
      "Random Pruning Iteration: 26\n",
      "Number of Zero Weights: 501647.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9969771527601119\n",
      "Test Accuracy: 0.2516\n",
      "Random Pruning Iteration: 27\n",
      "Number of Zero Weights: 501951.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9975813247265327\n",
      "Test Accuracy: 0.2023\n",
      "Random Pruning Iteration: 28\n",
      "Number of Zero Weights: 502194.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9980642648181124\n",
      "Test Accuracy: 0.1711\n",
      "Random Pruning Iteration: 29\n",
      "Number of Zero Weights: 502389.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9984518093360468\n",
      "Test Accuracy: 0.1426\n",
      "Random Pruning Iteration: 30\n",
      "Number of Zero Weights: 502545.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9987618449503943\n",
      "Test Accuracy: 0.1498\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3,100)\n",
    "    RandomPruning(model, 'RandomPruning_CIFAR100_seed' + str(i), path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 120, amount = .2, save_model = False, seed = i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
