{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee04858-9ce2-4b0f-a0cd-00d117251eb8",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf200b-4087-4836-a422-0630818bff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install kornia\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ecf34-d1ab-4285-beb2-0aaf3e5bdb60",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:00.691279Z",
     "iopub.status.busy": "2023-05-31T18:20:00.690678Z",
     "iopub.status.idle": "2023-05-31T18:20:11.792732Z",
     "shell.execute_reply": "2023-05-31T18:20:11.791912Z",
     "shell.execute_reply.started": "2023-05-31T18:20:00.691257Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import resnet50, resnet34, resnet18, wide_resnet50_2, ResNet50_Weights, alexnet\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from flax.training import checkpoints\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "#Import from MTT code\n",
    "from networks import ConvNet, AlexNet\n",
    "from distill import ParamDiffAug\n",
    "from utils import evaluate_synset, get_network\n",
    "import argparse\n",
    "\n",
    "import optuna\n",
    "import kornia\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a4af9-0b34-45f9-bf6f-85f3ae6cadb7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131a0bf1-a86b-4af0-9767-54de55893237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:11.796077Z",
     "iopub.status.busy": "2023-05-31T18:20:11.795927Z",
     "iopub.status.idle": "2023-05-31T18:20:11.871066Z",
     "shell.execute_reply": "2023-05-31T18:20:11.870340Z",
     "shell.execute_reply.started": "2023-05-31T18:20:11.796059Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load distilled data from MTT, if not in downloads directory, download from MTT repository: https://github.com/GeorgeCazenavette/mtt-distillation/\n",
    "labels_train = torch.load('./data/cifar100_50ipc_labels.pt')\n",
    "images_train = torch.load('./data/cifar100_50ipc_images.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7dd09d-22f0-43c7-8a0d-7afddd0ab18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:11.872662Z",
     "iopub.status.busy": "2023-05-31T18:20:11.872466Z",
     "iopub.status.idle": "2023-05-31T18:20:15.857947Z",
     "shell.execute_reply": "2023-05-31T18:20:15.857380Z",
     "shell.execute_reply.started": "2023-05-31T18:20:11.872646Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load in real training data from pytorch\n",
    "batch_size = 256\n",
    "train_dataset = torchvision.datasets.CIFAR100(root = './data',\n",
    "                                                    train = True,\n",
    "                                                    transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),]),\n",
    "                                                    download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(root = './data',\n",
    "                                                    train = False,\n",
    "                                                    transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),]),\n",
    "                                                    download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594dc2db-0fd3-4a18-904d-94aa89e8e5c7",
   "metadata": {},
   "source": [
    "# Methods used for Distilled Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "893abb26-d2bb-406a-ab58-9d68cf9c6438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:15.859459Z",
     "iopub.status.busy": "2023-05-31T18:20:15.859097Z",
     "iopub.status.idle": "2023-05-31T18:20:15.884642Z",
     "shell.execute_reply": "2023-05-31T18:20:15.884226Z",
     "shell.execute_reply.started": "2023-05-31T18:20:15.859437Z"
    }
   },
   "outputs": [],
   "source": [
    "#Standard train function with hyperparameters used in paper set as default\n",
    "def train(model,train_loader, num_epochs, lr = .0008, weight_decay = .0008, gamma = .15, milestones = [50,65,80]):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma= gamma)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = cost(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "    pass\n",
    "\n",
    "#Standard test function, prints & returns test accuracy\n",
    "def test(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader): \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        accuracy = correct / total\n",
    "\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "#Helper function for prunable all pruning modules to work pytorch global pruning. \n",
    "#See global pruning section of this: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\n",
    "def get_parameters_to_prune(model):\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    return tuple(parameters_to_prune)\n",
    "\n",
    "#Returns number of zeros and total number of prunable parameters of a model. Global Sparsity measured as: zero / total\n",
    "def sparsity_print(model):\n",
    "    prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=0)\n",
    "    zero = total = 0\n",
    "    for module, _ in get_parameters_to_prune(model):\n",
    "        zero += float(torch.sum(module.weight == 0))\n",
    "        total += float(module.weight.nelement())\n",
    "    print('Number of Zero Weights:', zero)\n",
    "    print('Total Number of Weights:', total)\n",
    "    print('Sparsity', zero/total)\n",
    "    #TODO: Implement Node Sparsity\n",
    "    return zero, total\n",
    "\n",
    "#Standard IMP with Weight Rewinding to the kth epoch in training, \n",
    "#name: a string that allows us to save models/logs appropriately, \n",
    "#path: the location of folder we save to,\n",
    "#start_iter: should normally be 0 but if a experiment stops halfway through it allows us to begin there,\n",
    "#amount = % params pruned each pruning iteration, \n",
    "#save_model: boolean to decide if we download the model at every iter, \n",
    "#reinit: is boolean value if we want to test results on reinitialized weights\n",
    "#reinit_model: is the specific model that holds the reinitialized weights, must be same type as model\n",
    "#seed: seed for pruning sequence, SEED IS NOT FOR MODEL INITIALIZATION FROM PAPER. We instead set seed outside the function for that.\n",
    "#num_epochs: number of epochs used for training\n",
    "def LotteryTicketRewinding(model, name, path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, k = 1, amount = .2, save_model = True, seed = 0, reinit = False, reinit_model = None):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = [] #keeps track of zeros at each iteration\n",
    "    totals = [] #keeps track of total parameters\n",
    "    acc = [] #keeps track of model accuracy at each pruning iteration\n",
    "    reinit_acc = [] #same as above but for reinitialized model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    #Create Rewind Weights after training K epochs\n",
    "    train(model, train_loader,num_epochs = k)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    #Finish off the pretraining\n",
    "    train(model, train_loader,num_epochs = num_epochs - k)\n",
    "\n",
    "    #Lottery Ticket Rewinding: Prune, Rewind, Train\n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('LTR Iteration:', i+1)\n",
    "        #Prune\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "        #Train Weights\n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        \n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        #Reinitialize the weights, train and validate on those new weights\n",
    "        if reinit:\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals,reinit_acc]))\n",
    "    \n",
    "    pass\n",
    "  \n",
    "#Generate full sparsity curve with retraining for random pruning, this is not a method to compute a single, high-sparsity model with random pruning. \n",
    "#This generates and trains models at all sparsities for comparison. \n",
    "#If you want to perform a single random pruning, then just use: prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.RandomUnstructured,amount=amount)\n",
    "def RandomPruning(model, name, path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, amount = .2, save_model = True, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    acc = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('Random Pruning Iteration:', i+1)\n",
    "        #Prune\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.RandomUnstructured,amount=amount)\n",
    "        #Train Weights\n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        \n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "\n",
    "            \n",
    "        #Rewind Weights to save them\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "                \n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals]))\n",
    "    \n",
    "#Distilled Pruning \n",
    "#See LotteryTicketRewinding for parameter description. \n",
    "#num_epochs_distilled: number of epochs used for distilled training\n",
    "#distilled_lr = learning rate for distilled training\n",
    "#validate = Retrains pruned model at each iteration on real data. This is used to generate a full sparsity curve, but is not efficient for finding a single distilled mask.\n",
    "#use validate false, and set end_iter to desired final iteration of the mask. Remember sparsity = 1-(1-amount)^end_iter\n",
    "def DistilledPruning(model, name, path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs_distilled = 1000, num_epochs_real = 60, k = 0, amount = .2, save_model = True, validate = False, seed = 0, reinit = False, reinit_model = None, distilled_lr = .01):\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    accs = []\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    reinit_acc = []\n",
    "    \n",
    "    #Create rewind weights at initailization\n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    \n",
    "    #Use if you want to try rewinding to an early point in training, this does not work well, so we suggest k=0 always.\n",
    "    if k != 0:\n",
    "        args = argparse.Namespace(lr_net=str(distilled_lr), device='cuda', epoch_eval_train=str(k),batch_train=512,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "        model_rewind, acc_train_list, acc_test = evaluate_synset(0, model_rewind,images_train,labels_train,test_loader,args)\n",
    "        \n",
    "    \n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('Distilled Pruning Iteration ', i)\n",
    "        #Set distilled pruning training args for MTT eval\n",
    "        args = argparse.Namespace(lr_net='.01', device='cuda', epoch_eval_train=str(num_epochs_distilled),batch_train=512,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "        #MTT Training on Distilled Data\n",
    "        model, acc_train_list, acc_test = evaluate_synset(i+1, model,images_train,labels_train,test_loader,args)\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "    \n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        #Rewind weights back to initialization and train on real data to validate this sparsity mask\n",
    "        if validate:\n",
    "            train(model, train_loader,num_epochs = num_epochs_real)\n",
    "            accs.append(test(model, test_loader))\n",
    "            zero, total = sparsity_print(model)\n",
    "            zeros.append(zero)\n",
    "            totals.append(total)\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "                    \n",
    "            np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n",
    "        \n",
    "        if reinit:\n",
    "            #Rewind Weights to Reinit Model\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs_real)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "            np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "    #If validate = False, then we still want to validate the final sparsity mask. just not all the masks.\n",
    "    if not validate:\n",
    "        train(model, train_loader,num_epochs = num_epochs_real)\n",
    "        acc = (test(model, test_loader))\n",
    "        zero, total = sparsity_print(model)\n",
    "        np.save(path + name + '_log', np.array([acc, zero, total, reinit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57203403-e2aa-4eae-a40e-3f40172945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save logs and models here\n",
    "path = './model_results_cifar100/' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f51b0f-23af-4196-a0e7-61cf69325e3b",
   "metadata": {},
   "source": [
    "For CIFAR 100, Distilled Pruning Hyperparameters = .085 distilled_lr and 1300 num_epochs_distilled. All else is in code and the same across experiments/datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e3c70-0bda-4973-8f31-579b6faea4c4",
   "metadata": {},
   "source": [
    "# Example Distilled Training Snippet for MTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c885a-47f1-49b3-8cff-97b6a057898e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T16:29:40.688940Z",
     "iopub.status.busy": "2023-05-30T16:29:40.688301Z",
     "iopub.status.idle": "2023-05-30T16:31:14.136578Z",
     "shell.execute_reply": "2023-05-30T16:31:14.135932Z",
     "shell.execute_reply.started": "2023-05-30T16:29:40.688914Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_network('ConvNetW128', 3, 100)\n",
    "args = argparse.Namespace(lr_net='.09', device='cuda', epoch_eval_train=str(1300),batch_train=512,dataset='cifar100',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "model, acc_train_list, acc_test = evaluate_synset(1, model,images_train,labels_train,test_loader,args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c4635-85bd-4b61-b0a5-475f975b25fa",
   "metadata": {},
   "source": [
    "# Example Distilled Pruning for computing a single sparsity mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a9c03-806d-41df-8733-5b83c6f092fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = get_network('ConvNetW128', 3, 100)\n",
    "DistilledPruning(model, 'DistilledPruning_CIFAR100_ipc50_seed0_iter5', path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 5, num_epochs_distilled = 1300, num_epochs_real = 120, k = 0, amount = .2, save_model = False, validate = True, seed = 0, reinit = False, reinit_model = None, distilled_lr = .09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2d06e-5797-4f6d-a76c-5505413784d5",
   "metadata": {},
   "source": [
    "# Distilled Pruning Experiment for generating plots / validating all sparsity mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9478a-58a5-4e26-9289-90a36529db4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:20:33.240209Z",
     "iopub.status.busy": "2023-05-31T18:20:33.239478Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3, 100)\n",
    "    DistilledPruning(model, 'DistilledPruning_CIFAR100_ipc50_seed' + str(i), path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 20, num_epochs_distilled = 1300, num_epochs_real = 120, k = 0, amount = .2, save_model = False, validate = True, seed = 0, reinit = False, reinit_model = None, distilled_lr = .09)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7dd5f-18cc-4691-a55e-e6b889bfbad3",
   "metadata": {},
   "source": [
    "# IMP Pruning Experiment for generating plots / validating all sparsity mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1763be-3343-44f1-b2ad-3897c4c1adfe",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-26T02:40:42.664587Z",
     "iopub.status.busy": "2023-05-26T02:40:42.664303Z",
     "iopub.status.idle": "2023-05-26T06:26:45.241275Z",
     "shell.execute_reply": "2023-05-26T06:26:45.240325Z",
     "shell.execute_reply.started": "2023-05-26T02:40:42.664566Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3,10)\n",
    "    LotteryTicketRewinding(model, 'IMP_CIFAR100_seed' + str(i), path, train_loader, test_loader, start_iter = 0, end_iter = 20, num_epochs = 120, k = 1, amount = .2, save_model = False, seed = i, reinit = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188de44-d388-4a93-97b0-bd9c53f9f7f7",
   "metadata": {},
   "source": [
    "# Random Pruning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a75ef5b-b092-4069-ad01-997f9305da6a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-05-25T22:14:54.956970Z",
     "iopub.status.busy": "2023-05-25T22:14:54.956771Z",
     "iopub.status.idle": "2023-05-26T01:53:25.340736Z",
     "shell.execute_reply": "2023-05-26T01:53:25.340058Z",
     "shell.execute_reply.started": "2023-05-25T22:14:54.956949Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3,100)\n",
    "    RandomPruning(model, 'RandomPruning_CIFAR100_seed' + str(i), path, train_loader, test_loader, start_iter = 0, end_iter = 20, num_epochs = 120, amount = .2, save_model = False, seed = i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
