{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f51ecf34-d1ab-4285-beb2-0aaf3e5bdb60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T07:09:01.678410Z",
     "iopub.status.busy": "2023-06-01T07:09:01.677853Z",
     "iopub.status.idle": "2023-06-01T07:09:09.677781Z",
     "shell.execute_reply": "2023-06-01T07:09:09.677059Z",
     "shell.execute_reply.started": "2023-06-01T07:09:01.678387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.13.4)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.30)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (66.1.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: kornia in /usr/local/lib/python3.9/dist-packages (0.6.12)\n",
      "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from kornia) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from kornia) (23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.1->kornia) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: optuna in /usr/local/lib/python3.9/dist-packages (3.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from optuna) (1.23.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.11.1)\n",
      "Requirement already satisfied: cmaes>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from optuna) (0.9.1)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.9/dist-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (23.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from optuna) (1.4.41)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from optuna) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (4.4.0)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import resnet50, resnet34, resnet18, wide_resnet50_2, ResNet50_Weights, alexnet\n",
    "import gc\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from flax.training import checkpoints\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n",
    "!pip install wandb\n",
    "!pip install kornia\n",
    "!pip install optuna\n",
    "import optuna\n",
    "import kornia\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131a0bf1-a86b-4af0-9767-54de55893237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T07:09:09.679655Z",
     "iopub.status.busy": "2023-06-01T07:09:09.679204Z",
     "iopub.status.idle": "2023-06-01T07:09:09.685401Z",
     "shell.execute_reply": "2023-06-01T07:09:09.684900Z",
     "shell.execute_reply.started": "2023-06-01T07:09:09.679633Z"
    }
   },
   "outputs": [],
   "source": [
    "from networks import ConvNet, AlexNet\n",
    "from distill import ParamDiffAug\n",
    "from utils import evaluate_synset, get_network\n",
    "import argparse\n",
    "#labels_train = torch.load('/datasets/mtt-cifar10-50-ipc/cifar10_50ipc_labels.pt')\n",
    "#images_train = torch.load('/datasets/mtt-cifar10-50-ipc/cifar10_50ipc_images.pt')\n",
    "labels_train = torch.load('./data/cifar10_10ipc_labels.pt')\n",
    "images_train = torch.load('./data/cifar10_10ipc_images.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe7dd09d-22f0-43c7-8a0d-7afddd0ab18e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T07:09:09.686293Z",
     "iopub.status.busy": "2023-06-01T07:09:09.686125Z",
     "iopub.status.idle": "2023-06-01T07:09:12.285235Z",
     "shell.execute_reply": "2023-06-01T07:09:12.284639Z",
     "shell.execute_reply.started": "2023-06-01T07:09:09.686278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                                    train = True,\n",
    "                                                    transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),]),\n",
    "                                                    download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                                    train = False,\n",
    "                                                    transform = transforms.Compose([\n",
    "                                                            transforms.ToTensor(),\n",
    "                                                            transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]),]),\n",
    "                                                    download=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "transform_train = transforms.Compose([transforms.Resize((32,32)),  #resises the image so it can be perfect for our model.\n",
    "                                      transforms.RandomHorizontalFlip(), # FLips the image w.r.t horizontal axis\n",
    "                                      transforms.RandomRotation(10),     #Rotates the image to a specified angel\n",
    "                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)), #Performs actions like zooms, change shear angles.\n",
    "                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Set the color params\n",
    "                                      transforms.ToTensor(), # comvert the image to tensor so that it can work with torch\n",
    "                                      transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2470, 0.2435, 0.2616]), #Normalize all the images\n",
    "                               ])\n",
    "\n",
    "\n",
    "augmented_train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
    "                                                    train = True,\n",
    "                                                    transform = transform_train,\n",
    "                                                    download=True)\n",
    "\n",
    "augmented_train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f51b0f-23af-4196-a0e7-61cf69325e3b",
   "metadata": {},
   "source": [
    "Distilled Training Params: .085 lr and 1300 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff0fa24-5945-4949-bed1-2454f4c0d268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T17:30:34.141905Z",
     "iopub.status.busy": "2023-05-31T17:30:34.141736Z",
     "iopub.status.idle": "2023-05-31T17:32:01.765911Z",
     "shell.execute_reply": "2023-05-31T17:32:01.765338Z",
     "shell.execute_reply.started": "2023-05-31T17:30:34.141889Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get pretrained model to determine what is easy/hard\n",
    "pretrained_model = AlexNet(3,10).to('cuda')\n",
    "train(pretrained_model, test_loader, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7620b4b2-a6b9-4e28-88e9-e4065487720b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T02:14:26.287090Z",
     "iopub.status.busy": "2023-05-30T02:14:26.286320Z",
     "iopub.status.idle": "2023-05-30T02:17:18.681743Z",
     "shell.execute_reply": "2023-05-30T02:17:18.680927Z",
     "shell.execute_reply.started": "2023-05-30T02:14:26.287068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 58.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:14:34] Evaluate_01: epoch = 0500 train time = 8 s train loss = 2.299133 train acc = 0.0700, test acc = 0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 59.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:14:43] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.348136 train acc = 0.9000, test acc = 0.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 57.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:14:51] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.011322 train acc = 1.0000, test acc = 0.3062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:09<00:00, 53.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:01] Evaluate_01: epoch = 0500 train time = 9 s train loss = 0.000739 train acc = 1.0000, test acc = 0.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 57.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:09] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.003284 train acc = 1.0000, test acc = 0.3203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 58.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:18] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.009369 train acc = 1.0000, test acc = 0.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 61.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:26] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000163 train acc = 1.0000, test acc = 0.3254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 58.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:35] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000531 train acc = 1.0000, test acc = 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 56.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:44] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.001921 train acc = 1.0000, test acc = 0.3241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:09<00:00, 54.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:15:53] Evaluate_01: epoch = 0500 train time = 9 s train loss = 0.000930 train acc = 1.0000, test acc = 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:01] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000200 train acc = 1.0000, test acc = 0.3237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 61.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:09] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000074 train acc = 1.0000, test acc = 0.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 61.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:18] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000148 train acc = 1.0000, test acc = 0.3228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 58.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:26] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000066 train acc = 1.0000, test acc = 0.3223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 56.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:35] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000042 train acc = 1.0000, test acc = 0.3257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 59.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:43] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000103 train acc = 1.0000, test acc = 0.3212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 58.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:16:52] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000100 train acc = 1.0000, test acc = 0.3219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 60.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:17:00] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000427 train acc = 1.0000, test acc = 0.3248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 56.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:17:09] Evaluate_01: epoch = 0500 train time = 8 s train loss = 0.000306 train acc = 1.0000, test acc = 0.3237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:09<00:00, 54.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:17:18] Evaluate_01: epoch = 0500 train time = 9 s train loss = 0.000072 train acc = 1.0000, test acc = 0.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet(3,10).to('cuda')\n",
    "args = argparse.Namespace(lr_net=str(.007), device='cuda', epoch_eval_train=str(500),batch_train=100,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True))                        \n",
    "for i in range(20):\n",
    "    model, acc_train_list, acc_test = evaluate_synset(1, model,images_train,labels_train,test_loader,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b423c72d-ea92-4904-a534-ecc4777513ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-30T01:58:50.790464Z",
     "iopub.status.busy": "2023-05-30T01:58:50.789793Z",
     "iopub.status.idle": "2023-05-30T02:12:16.028812Z",
     "shell.execute_reply": "2023-05-30T02:12:16.027640Z",
     "shell.execute_reply.started": "2023-05-30T01:58:50.790440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-30 01:58:50,793]\u001b[0m A new study created in memory with name: no-name-37b6b398-9c9f-458e-b060-89c4733957d3\u001b[0m\n",
      "100%|██████████| 3001/3001 [00:42<00:00, 70.97it/s]\n",
      "\u001b[32m[I 2023-05-30 01:59:33,096]\u001b[0m Trial 0 finished with value: 0.3284 and parameters: {'learning_rate': 0.02310179288511339}. Best is trial 0 with value: 0.3284.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 01:59:33] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.000437 train acc = 1.0000, test acc = 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 70.59it/s]\n",
      "\u001b[32m[I 2023-05-30 02:00:15,627]\u001b[0m Trial 1 finished with value: 0.3269 and parameters: {'learning_rate': 0.018072629718177063}. Best is trial 0 with value: 0.3284.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:00:15] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.000784 train acc = 1.0000, test acc = 0.3269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 70.23it/s]\n",
      "\u001b[32m[I 2023-05-30 02:00:58,372]\u001b[0m Trial 2 finished with value: 0.1 and parameters: {'learning_rate': 0.0372786551854656}. Best is trial 0 with value: 0.3284.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:00:58] Evaluate_01: epoch = 3000 train time = 42 s train loss = nan train acc = 0.1000, test acc = 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.41it/s]\n",
      "\u001b[32m[I 2023-05-30 02:01:41,621]\u001b[0m Trial 3 finished with value: 0.3135 and parameters: {'learning_rate': 0.01663794574883109}. Best is trial 0 with value: 0.3284.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:01:41] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.006120 train acc = 1.0000, test acc = 0.3135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 70.19it/s]\n",
      "\u001b[32m[I 2023-05-30 02:02:24,398]\u001b[0m Trial 4 finished with value: 0.3232 and parameters: {'learning_rate': 0.02303391407430934}. Best is trial 0 with value: 0.3284.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:02:24] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.000427 train acc = 1.0000, test acc = 0.3232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 68.71it/s]\n",
      "\u001b[32m[I 2023-05-30 02:03:08,092]\u001b[0m Trial 5 finished with value: 0.3202 and parameters: {'learning_rate': 0.03717307116327658}. Best is trial 0 with value: 0.3284.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:03:08] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.000490 train acc = 1.0000, test acc = 0.3202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.21it/s]\n",
      "\u001b[32m[I 2023-05-30 02:03:51,466]\u001b[0m Trial 6 finished with value: 0.3298 and parameters: {'learning_rate': 0.015936678260982853}. Best is trial 6 with value: 0.3298.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:03:51] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.000756 train acc = 1.0000, test acc = 0.3298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 70.31it/s]\n",
      "\u001b[32m[I 2023-05-30 02:04:34,166]\u001b[0m Trial 7 finished with value: 0.3341 and parameters: {'learning_rate': 0.0242413892433275}. Best is trial 7 with value: 0.3341.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:04:34] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.000416 train acc = 1.0000, test acc = 0.3341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:44<00:00, 67.12it/s]\n",
      "\u001b[32m[I 2023-05-30 02:05:18,892]\u001b[0m Trial 8 finished with value: 0.3384 and parameters: {'learning_rate': 0.013692409636334151}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:05:18] Evaluate_01: epoch = 3000 train time = 44 s train loss = 0.002286 train acc = 1.0000, test acc = 0.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.38it/s]\n",
      "\u001b[32m[I 2023-05-30 02:06:02,162]\u001b[0m Trial 9 finished with value: 0.3227 and parameters: {'learning_rate': 0.012011444326265639}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:06:02] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.001083 train acc = 1.0000, test acc = 0.3227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 70.16it/s]\n",
      "\u001b[32m[I 2023-05-30 02:06:44,955]\u001b[0m Trial 10 finished with value: 0.2881 and parameters: {'learning_rate': 0.0010649146182848332}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:06:44] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.129216 train acc = 0.9600, test acc = 0.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.36it/s]\n",
      "\u001b[32m[I 2023-05-30 02:07:28,242]\u001b[0m Trial 11 finished with value: 0.2964 and parameters: {'learning_rate': 0.028058313249173655}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:07:28] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.000148 train acc = 1.0000, test acc = 0.2964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.33it/s]\n",
      "\u001b[32m[I 2023-05-30 02:08:11,548]\u001b[0m Trial 12 finished with value: 0.3292 and parameters: {'learning_rate': 0.010604131525815111}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:08:11] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.001578 train acc = 1.0000, test acc = 0.3292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.59it/s]\n",
      "\u001b[32m[I 2023-05-30 02:08:54,691]\u001b[0m Trial 13 finished with value: 0.3236 and parameters: {'learning_rate': 0.030983003579843}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:08:54] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.000247 train acc = 1.0000, test acc = 0.3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 70.35it/s]\n",
      "\u001b[32m[I 2023-05-30 02:09:37,367]\u001b[0m Trial 14 finished with value: 0.2997 and parameters: {'learning_rate': 0.027394023027264253}. Best is trial 8 with value: 0.3384.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:09:37] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.000968 train acc = 1.0000, test acc = 0.2997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 69.90it/s]\n",
      "\u001b[32m[I 2023-05-30 02:10:20,321]\u001b[0m Trial 15 finished with value: 0.3561 and parameters: {'learning_rate': 0.0073087471076532864}. Best is trial 15 with value: 0.3561.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:10:20] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.001057 train acc = 1.0000, test acc = 0.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:43<00:00, 69.70it/s]\n",
      "\u001b[32m[I 2023-05-30 02:11:03,401]\u001b[0m Trial 16 finished with value: 0.3374 and parameters: {'learning_rate': 0.005233835384086168}. Best is trial 15 with value: 0.3561.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:11:03] Evaluate_01: epoch = 3000 train time = 43 s train loss = 0.001276 train acc = 1.0000, test acc = 0.3374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:42<00:00, 71.43it/s]\n",
      "\u001b[32m[I 2023-05-30 02:11:45,433]\u001b[0m Trial 17 finished with value: 0.2966 and parameters: {'learning_rate': 0.009124869761263386}. Best is trial 15 with value: 0.3561.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-30 02:11:45] Evaluate_01: epoch = 3000 train time = 42 s train loss = 0.005263 train acc = 1.0000, test acc = 0.2966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 2092/3001 [00:30<00:13, 68.65it/s]\n",
      "\u001b[33m[W 2023-05-30 02:12:15,922]\u001b[0m Trial 18 failed with parameters: {'learning_rate': 0.006584268801467397} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_32/4138845070.py\", line 9, in objective\n",
      "    model, acc_train_list, acc_test = evaluate_synset(1, model,images_train,labels_train,test_loader,args)\n",
      "  File \"/notebooks/mtt_distillation/utils.py\", line 371, in evaluate_synset\n",
      "    loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug=True, texture=texture)\n",
      "  File \"/notebooks/mtt_distillation/utils.py\", line 334, in epoch\n",
      "    acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-30 02:12:15,925]\u001b[0m Trial 18 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc_test\n\u001b[1;32m     13\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler())\n\u001b[0;32m---> 14\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [13], line 9\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(lr_net\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, epoch_eval_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m3000\u001b[39m),batch_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcifar10\u001b[39m\u001b[38;5;124m'\u001b[39m,dsa\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,dsa_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolor_crop_cutout_flip_scale_rotate\u001b[39m\u001b[38;5;124m'\u001b[39m,dsa_param \u001b[38;5;241m=\u001b[39m ParamDiffAug(), dc_aug_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, zca_trans\u001b[38;5;241m=\u001b[39mkornia\u001b[38;5;241m.\u001b[39menhance\u001b[38;5;241m.\u001b[39mZCAWhitening(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, compute_inv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))                        \n\u001b[0;32m----> 9\u001b[0m model, acc_train_list, acc_test \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_synset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m acc_test\n",
      "File \u001b[0;32m/notebooks/mtt_distillation/utils.py:371\u001b[0m, in \u001b[0;36mevaluate_synset\u001b[0;34m(it_eval, net, images_train, labels_train, testloader, args, return_loss, texture)\u001b[0m\n\u001b[1;32m    368\u001b[0m loss_train_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(Epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m--> 371\u001b[0m     loss_train, acc_train \u001b[38;5;241m=\u001b[39m \u001b[43mepoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexture\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     acc_train_list\u001b[38;5;241m.\u001b[39mappend(acc_train)\n\u001b[1;32m    373\u001b[0m     loss_train_list\u001b[38;5;241m.\u001b[39mappend(loss_train)\n",
      "File \u001b[0;32m/notebooks/mtt_distillation/utils.py:334\u001b[0m, in \u001b[0;36mepoch\u001b[0;34m(mode, dataloader, net, optimizer, criterion, args, aug, texture)\u001b[0m\n\u001b[1;32m    331\u001b[0m output \u001b[38;5;241m=\u001b[39m net(img)\n\u001b[1;32m    332\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, lab)\n\u001b[0;32m--> 334\u001b[0m acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mequal(np\u001b[38;5;241m.\u001b[39margmax(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), lab\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[1;32m    336\u001b[0m loss_avg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mn_b\n\u001b[1;32m    337\u001b[0m acc_avg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "              'learning_rate': trial.suggest_float('learning_rate', 8e-4, 4e-2),\n",
    "              }\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    model = AlexNet(3,10).to('cuda')\n",
    "    args = argparse.Namespace(lr_net=str(params['learning_rate']), device='cuda', epoch_eval_train=str(3000),batch_train=100,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True))                        \n",
    "    model, acc_train_list, acc_test = evaluate_synset(1, model,images_train,labels_train,test_loader,args)\n",
    "\n",
    "    return acc_test\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e7c885a-47f1-49b3-8cff-97b6a057898e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T15:54:00.912636Z",
     "iopub.status.busy": "2023-05-26T15:54:00.911962Z",
     "iopub.status.idle": "2023-05-26T16:00:45.113627Z",
     "shell.execute_reply": "2023-05-26T16:00:45.112740Z",
     "shell.execute_reply.started": "2023-05-26T15:54:00.912608Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1301/1301 [06:44<00:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-26 16:00:45] Evaluate_01: epoch = 1300 train time = 404 s train loss = nan train acc = 0.0100, test acc = 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet(3,100).to('cuda')\n",
    "args = argparse.Namespace(lr_net='.01', device='cuda', epoch_eval_train=str(1300),batch_train=512,dataset='cifar100',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "model, acc_train_list, acc_test = evaluate_synset(1, model,images_train,labels_train,test_loader,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9478a-58a5-4e26-9289-90a36529db4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-01T07:09:41.605105Z",
     "iopub.status.busy": "2023-06-01T07:09:41.604847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled Pruning Iteration  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 3001/3001 [00:49<00:00, 60.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 07:10:31] Evaluate_01: epoch = 3000 train time = 49 s train loss = 0.000194 train acc = 1.0000, test acc = 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/1718027906.py:259: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8141\n",
      "Number of Zero Weights: 374263.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.2000088711608181\n",
      "Distilled Pruning Iteration  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 58.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 07:18:44] Evaluate_02: epoch = 3000 train time = 50 s train loss = 0.000677 train acc = 1.0000, test acc = 0.3083\n",
      "Test Accuracy: 0.8114\n",
      "Number of Zero Weights: 673664.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.36001094466105754\n",
      "Distilled Pruning Iteration  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 59.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 07:26:52] Evaluate_03: epoch = 3000 train time = 50 s train loss = 0.000118 train acc = 1.0000, test acc = 0.3161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:48<00:00, 61.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 07:35:05] Evaluate_04: epoch = 3000 train time = 48 s train loss = 0.001750 train acc = 1.0000, test acc = 0.3184\n",
      "Test Accuracy: 0.7843\n",
      "Number of Zero Weights: 1478806.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.7902846894452424\n",
      "Distilled Pruning Iteration  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:52<00:00, 57.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 08:07:50] Evaluate_08: epoch = 3000 train time = 52 s train loss = 0.000544 train acc = 1.0000, test acc = 0.3110\n",
      "Test Accuracy: 0.7918\n",
      "Number of Zero Weights: 1557292.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.8322281790820165\n",
      "Distilled Pruning Iteration  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:51<00:00, 57.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 08:15:59] Evaluate_09: epoch = 3000 train time = 51 s train loss = 0.000451 train acc = 1.0000, test acc = 0.3166\n",
      "Test Accuracy: 0.7924\n",
      "Number of Zero Weights: 1620079.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.865782008858335\n",
      "Distilled Pruning Iteration  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:48<00:00, 62.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 08:24:10] Evaluate_10: epoch = 3000 train time = 48 s train loss = 0.015479 train acc = 0.9900, test acc = 0.3014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 59.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 09:13:20] Evaluate_16: epoch = 3000 train time = 50 s train loss = 0.003733 train acc = 1.0000, test acc = 0.2952\n",
      "Test Accuracy: 0.7462\n",
      "Number of Zero Weights: 1837523.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9819856650591696\n",
      "Distilled Pruning Iteration  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 59.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 09:37:50] Evaluate_19: epoch = 3000 train time = 50 s train loss = 0.000861 train acc = 1.0000, test acc = 0.3001\n",
      "Test Accuracy: 0.7356\n",
      "Number of Zero Weights: 1844265.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9855886389287913\n",
      "Distilled Pruning Iteration  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:53<00:00, 56.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 09:46:05] Evaluate_20: epoch = 3000 train time = 53 s train loss = 0.000914 train acc = 1.0000, test acc = 0.3035\n",
      "Test Accuracy: 0.7304\n",
      "Number of Zero Weights: 1849658.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9884706973801217\n",
      "Distilled Pruning Iteration  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:52<00:00, 57.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 09:54:17] Evaluate_21: epoch = 3000 train time = 52 s train loss = 0.000670 train acc = 1.0000, test acc = 0.2994\n",
      "Test Accuracy: 0.7827\n",
      "Number of Zero Weights: 1380701.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.7378566634174704\n",
      "Distilled Pruning Iteration  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:47<00:00, 63.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 13:27:10] Evaluate_17: epoch = 3000 train time = 47 s train loss = 0.004730 train acc = 1.0000, test acc = 0.3028\n",
      "Test Accuracy: 0.7499\n",
      "Number of Zero Weights: 1829097.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9774827493330597\n",
      "Distilled Pruning Iteration  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:51<00:00, 58.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 13:35:23] Evaluate_18: epoch = 3000 train time = 51 s train loss = 0.000755 train acc = 1.0000, test acc = 0.3037\n",
      "Test Accuracy: 0.7462\n",
      "Number of Zero Weights: 1837523.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9819856650591696\n",
      "Distilled Pruning Iteration  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 59.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 13:43:31] Evaluate_19: epoch = 3000 train time = 50 s train loss = 0.001572 train acc = 1.0000, test acc = 0.3033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 59.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 13:51:46] Evaluate_20: epoch = 3000 train time = 50 s train loss = 0.001422 train acc = 1.0000, test acc = 0.3019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:50<00:00, 59.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 14:08:11] Evaluate_22: epoch = 3000 train time = 50 s train loss = 0.000341 train acc = 1.0000, test acc = 0.2880\n",
      "Test Accuracy: 0.6748\n",
      "Number of Zero Weights: 1857425.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.992621438709898\n",
      "Distilled Pruning Iteration  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:51<00:00, 58.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 14:16:19] Evaluate_23: epoch = 3000 train time = 51 s train loss = 0.003557 train acc = 1.0000, test acc = 0.2869\n",
      "Test Accuracy: 0.6686\n",
      "Number of Zero Weights: 1860187.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9940974716122853\n",
      "Distilled Pruning Iteration  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 2530/3001 [00:38<00:09, 49.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6429\n",
      "Number of Zero Weights: 1862395.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9952774428825502\n",
      "Distilled Pruning Iteration  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 79/3001 [00:01<00:50, 57.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5869\n",
      "Number of Zero Weights: 1864162.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9962217405431288\n",
      "Distilled Pruning Iteration  25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:46<00:00, 64.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 14:40:54] Evaluate_26: epoch = 3000 train time = 46 s train loss = 2.302458 train acc = 0.1400, test acc = 0.1659\n",
      "Test Accuracy: 0.1\n",
      "Number of Zero Weights: 1867612.0\n",
      "Total Number of Weights: 1871232.0\n",
      "Sparsity 0.9980654456529174\n",
      "Distilled Pruning Iteration  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [00:31<00:00, 95.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-01 15:04:22] Evaluate_29: epoch = 3000 train time = 31 s train loss = 2.302585 train acc = 0.1000, test acc = 0.1000\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,5):\n",
    "        torch.manual_seed(i)\n",
    "        model = AlexNet(3,10)\n",
    "        DistilledPruning(model, 'dist_c10_ipc10_an_seed' + str(i), path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs_distilled = 3000, num_epochs_real = 60, k = 0, amount = .2, save_model = False, validate = True, seed = 0, reinit = False, reinit_model = None, distilled_lr = .007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1763be-3343-44f1-b2ad-3897c4c1adfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-26T02:40:42.664587Z",
     "iopub.status.busy": "2023-05-26T02:40:42.664303Z",
     "iopub.status.idle": "2023-05-26T06:26:45.241275Z",
     "shell.execute_reply": "2023-05-26T06:26:45.240325Z",
     "shell.execute_reply.started": "2023-05-26T02:40:42.664566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTR Iteration: 1\n",
      "Number of Zero Weights: 63770.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.20000125451625853\n",
      "Test Accuracy: 0.8143\n",
      "LTR Iteration: 2\n",
      "Number of Zero Weights: 114786.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.36000225812926534\n",
      "Test Accuracy: 0.8132\n",
      "LTR Iteration: 3\n",
      "Number of Zero Weights: 155598.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.48800055198715375\n",
      "Test Accuracy: 0.8118\n",
      "LTR Iteration: 4\n",
      "Number of Zero Weights: 188248.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.590400441589723\n",
      "Test Accuracy: 0.8094\n",
      "LTR Iteration: 5\n",
      "Number of Zero Weights: 214368.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.6723203532717784\n",
      "Test Accuracy: 0.8101\n",
      "LTR Iteration: 6\n",
      "Number of Zero Weights: 235264.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.7378562826174228\n",
      "Test Accuracy: 0.806\n",
      "LTR Iteration: 7\n",
      "Number of Zero Weights: 251981.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.7902856533520675\n",
      "Test Accuracy: 0.8042\n",
      "LTR Iteration: 8\n",
      "Number of Zero Weights: 265354.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.8322272681653954\n",
      "Test Accuracy: 0.8029\n",
      "LTR Iteration: 9\n",
      "Number of Zero Weights: 276053.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.8657824417904456\n",
      "Test Accuracy: 0.7962\n",
      "LTR Iteration: 10\n",
      "Number of Zero Weights: 284612.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.8926259534323565\n",
      "Test Accuracy: 0.7945\n",
      "LTR Iteration: 11\n",
      "Number of Zero Weights: 291459.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9141001354877559\n",
      "Test Accuracy: 0.7851\n",
      "LTR Iteration: 12\n",
      "Number of Zero Weights: 296937.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.931280735648334\n",
      "Test Accuracy: 0.7749\n",
      "LTR Iteration: 13\n",
      "Number of Zero Weights: 301319.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9450239612605379\n",
      "Test Accuracy: 0.7725\n",
      "LTR Iteration: 14\n",
      "Number of Zero Weights: 304825.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9560197962665596\n",
      "Test Accuracy: 0.7663\n",
      "LTR Iteration: 15\n",
      "Number of Zero Weights: 307630.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9648170915295062\n",
      "Test Accuracy: 0.7679\n",
      "LTR Iteration: 16\n",
      "Number of Zero Weights: 309874.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9718549277398635\n",
      "Test Accuracy: 0.757\n",
      "LTR Iteration: 17\n",
      "Number of Zero Weights: 311669.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.97748456945002\n",
      "Test Accuracy: 0.751\n",
      "LTR Iteration: 18\n",
      "Number of Zero Weights: 313105.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9819882828181453\n",
      "Test Accuracy: 0.7341\n",
      "LTR Iteration: 19\n",
      "Number of Zero Weights: 314254.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9855918807707748\n",
      "Test Accuracy: 0.7274\n",
      "LTR Iteration: 20\n",
      "Number of Zero Weights: 315173.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9884741318747491\n",
      "Test Accuracy: 0.7071\n",
      "LTR Iteration: 21\n",
      "Number of Zero Weights: 315908.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9907793054997993\n",
      "Test Accuracy: 0.6792\n",
      "LTR Iteration: 22\n",
      "Number of Zero Weights: 316496.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9926234443998394\n",
      "Test Accuracy: 0.6619\n",
      "LTR Iteration: 23\n",
      "Number of Zero Weights: 316966.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.994097501003613\n",
      "Test Accuracy: 0.6368\n",
      "LTR Iteration: 24\n",
      "Number of Zero Weights: 317342.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9952767462866319\n",
      "Test Accuracy: 0.628\n",
      "LTR Iteration: 25\n",
      "Number of Zero Weights: 317643.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9962207697711762\n",
      "Test Accuracy: 0.5611\n",
      "LTR Iteration: 26\n",
      "Number of Zero Weights: 317884.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.996976615816941\n",
      "Test Accuracy: 0.5451\n",
      "LTR Iteration: 27\n",
      "Number of Zero Weights: 318077.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9975819199116821\n",
      "Test Accuracy: 0.5434\n",
      "LTR Iteration: 28\n",
      "Number of Zero Weights: 318231.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9980649086712163\n",
      "Test Accuracy: 0.4777\n",
      "LTR Iteration: 29\n",
      "Number of Zero Weights: 318354.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9984506724207146\n",
      "Test Accuracy: 0.461\n",
      "LTR Iteration: 30\n",
      "Number of Zero Weights: 318453.0\n",
      "Total Number of Weights: 318848.0\n",
      "Sparsity 0.9987611651947009\n",
      "Test Accuracy: 0.4082\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3,10)\n",
    "    LotteryTicketRewinding(model, 'ltr_c10_seed' + str(i), path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, k = 1, amount = .2, save_model = False, seed = i, reinit = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a75ef5b-b092-4069-ad01-997f9305da6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-25T22:14:54.956970Z",
     "iopub.status.busy": "2023-05-25T22:14:54.956771Z",
     "iopub.status.idle": "2023-05-26T01:53:25.340736Z",
     "shell.execute_reply": "2023-05-26T01:53:25.340058Z",
     "shell.execute_reply.started": "2023-05-25T22:14:54.956949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Pruning Iteration: 1\n",
      "Number of Zero Weights: 100634.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.2000007949631137\n",
      "Test Accuracy: 0.8126\n",
      "Random Pruning Iteration: 2\n",
      "Number of Zero Weights: 181141.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.3600010334520478\n",
      "Test Accuracy: 0.7985\n",
      "Random Pruning Iteration: 3\n",
      "Number of Zero Weights: 245547.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.4880020192063088\n",
      "Test Accuracy: 0.7913\n",
      "Random Pruning Iteration: 4\n",
      "Number of Zero Weights: 297070.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.590399230475706\n",
      "Test Accuracy: 0.781\n",
      "Random Pruning Iteration: 5\n",
      "Number of Zero Weights: 338290.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.6723201793436785\n",
      "Test Accuracy: 0.7751\n",
      "Random Pruning Iteration: 6\n",
      "Number of Zero Weights: 371266.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.7378569384380564\n",
      "Test Accuracy: 0.7669\n",
      "Random Pruning Iteration: 7\n",
      "Number of Zero Weights: 397646.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.7902847557873315\n",
      "Test Accuracy: 0.7684\n",
      "Random Pruning Iteration: 8\n",
      "Number of Zero Weights: 418750.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.8322270096667514\n",
      "Test Accuracy: 0.7458\n",
      "Random Pruning Iteration: 9\n",
      "Number of Zero Weights: 435634.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.8657824026965149\n",
      "Test Accuracy: 0.7493\n",
      "Random Pruning Iteration: 10\n",
      "Number of Zero Weights: 449141.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.8926263196387687\n",
      "Test Accuracy: 0.7298\n",
      "Random Pruning Iteration: 11\n",
      "Number of Zero Weights: 459946.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9141002607479013\n",
      "Test Accuracy: 0.727\n",
      "Random Pruning Iteration: 12\n",
      "Number of Zero Weights: 468590.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9312794136352073\n",
      "Test Accuracy: 0.7261\n",
      "Random Pruning Iteration: 13\n",
      "Number of Zero Weights: 475506.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9450243258712796\n",
      "Test Accuracy: 0.7153\n",
      "Random Pruning Iteration: 14\n",
      "Number of Zero Weights: 481038.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9560186657339099\n",
      "Test Accuracy: 0.6958\n",
      "Random Pruning Iteration: 15\n",
      "Number of Zero Weights: 485464.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9648149325871279\n",
      "Test Accuracy: 0.6827\n",
      "Random Pruning Iteration: 16\n",
      "Number of Zero Weights: 489005.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9718523435512593\n",
      "Test Accuracy: 0.6704\n",
      "Random Pruning Iteration: 17\n",
      "Number of Zero Weights: 491838.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9774826698041211\n",
      "Test Accuracy: 0.6413\n",
      "Random Pruning Iteration: 18\n",
      "Number of Zero Weights: 494104.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9819861358432969\n",
      "Test Accuracy: 0.6195\n",
      "Random Pruning Iteration: 19\n",
      "Number of Zero Weights: 495917.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9855893061561943\n",
      "Test Accuracy: 0.5957\n",
      "Random Pruning Iteration: 20\n",
      "Number of Zero Weights: 497367.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9884710474433986\n",
      "Test Accuracy: 0.5452\n",
      "Random Pruning Iteration: 21\n",
      "Number of Zero Weights: 498527.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.990776440473162\n",
      "Test Accuracy: 0.5109\n",
      "Random Pruning Iteration: 22\n",
      "Number of Zero Weights: 499455.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9926207548969728\n",
      "Test Accuracy: 0.4642\n",
      "Random Pruning Iteration: 23\n",
      "Number of Zero Weights: 500198.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9940973988806919\n",
      "Test Accuracy: 0.4061\n",
      "Random Pruning Iteration: 24\n",
      "Number of Zero Weights: 500792.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9952779191045535\n",
      "Test Accuracy: 0.3375\n",
      "Random Pruning Iteration: 25\n",
      "Number of Zero Weights: 501267.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.996221937802086\n",
      "Test Accuracy: 0.3026\n",
      "Random Pruning Iteration: 26\n",
      "Number of Zero Weights: 501647.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9969771527601119\n",
      "Test Accuracy: 0.2516\n",
      "Random Pruning Iteration: 27\n",
      "Number of Zero Weights: 501951.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9975813247265327\n",
      "Test Accuracy: 0.2023\n",
      "Random Pruning Iteration: 28\n",
      "Number of Zero Weights: 502194.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9980642648181124\n",
      "Test Accuracy: 0.1711\n",
      "Random Pruning Iteration: 29\n",
      "Number of Zero Weights: 502389.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9984518093360468\n",
      "Test Accuracy: 0.1426\n",
      "Random Pruning Iteration: 30\n",
      "Number of Zero Weights: 502545.0\n",
      "Total Number of Weights: 503168.0\n",
      "Sparsity 0.9987618449503943\n",
      "Test Accuracy: 0.1498\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    torch.manual_seed(i)\n",
    "    model = get_network('ConvNetW128', 3,100)\n",
    "    RandomPruning(model, 'random_c10_seed' + str(i), path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, amount = .2, save_model = False, seed = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cd085f-5189-4ed5-9074-9ab264e0fe92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T17:32:01.767423Z",
     "iopub.status.busy": "2023-05-31T17:32:01.766853Z",
     "iopub.status.idle": "2023-05-31T17:32:18.943674Z",
     "shell.execute_reply": "2023-05-31T17:32:18.943196Z",
     "shell.execute_reply.started": "2023-05-31T17:32:01.767393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\neasy_train_dataset_5000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='easy', num_samples=5000)\\n\\neasy_train_loader_5000 = torch.utils.data.DataLoader(dataset = easy_train_dataset_5000,\\n                                                    batch_size = batch_size,\\n                                                    shuffle = True)\\n\\nhard_train_dataset_5000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='hard', num_samples=5000)\\n\\nhard_train_loader_5000 = torch.utils.data.DataLoader(dataset = hard_train_dataset_5000,\\n                                                    batch_size = batch_size,\\n                                                    shuffle = True)\\n\\nrandom_train_dataset_5000 = RandomCIFAR10Dataset(root = './data', transform=None, target_transform=None, num_samples=5000)\\n\\nrandom_train_loader_5000 = torch.utils.data.DataLoader(dataset = random_train_dataset_5000,\\n                                                    batch_size = batch_size,\\n                                                    shuffle = True)\\n                                                    \\neasy_train_dataset_25000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='easy', num_samples=25000)\\n\\neasy_train_loader_25000 = torch.utils.data.DataLoader(dataset = easy_train_dataset_25000,\\n                                                    batch_size = batch_size,\\n                                                    shuffle = True)\\n\\nhard_train_dataset_25000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='hard', num_samples=25000)\\n\\nhard_train_loader_25000 = torch.utils.data.DataLoader(dataset = hard_train_dataset_25000,\\n                                                    batch_size = batch_size,\\n                                                    shuffle = True)\\n\\nrandom_train_dataset_25000 = RandomCIFAR10Dataset(root = './data', transform=None, target_transform=None, num_samples=25000)\\n\\nrandom_train_loader_25000 = torch.utils.data.DataLoader(dataset = random_train_dataset_25000,\\n                                                    batch_size = batch_size,\\n                                                    shuffle = True)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomCIFAR10Dataset(Dataset):\n",
    "    def __init__(self, root, transform=None, target_transform=None, num_samples=1000):\n",
    "        self.cifar = torchvision.datasets.CIFAR10(root, train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "        self.num_samples = num_samples\n",
    "        self.indices = torch.randperm(len(self.cifar))[:self.num_samples]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.cifar[self.indices[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "class EasyHardCIFAR10Dataset(Dataset):\n",
    "    def __init__(self, root, pretrained_model, dataloader, transform=None, target_transform=None, mode='easy', num_samples=1000):\n",
    "        assert mode in ['easy', 'hard'], \"Mode should be 'easy' or 'hard'\"\n",
    "        transform = transforms.Compose([transforms.ToTensor()]) if transform is None else transform\n",
    "        self.cifar = torchvision.datasets.CIFAR10(root, train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.mode = mode\n",
    "        self.num_samples = num_samples\n",
    "        self.dataloader = dataloader\n",
    "\n",
    "        self.pretrained_model.eval()\n",
    "        self.indices = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, targets in self.dataloader:\n",
    "                data = data.to('cuda')\n",
    "                targets = targets.to('cuda')\n",
    "                outputs = self.pretrained_model(data)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "\n",
    "                # Get confidence scores and sort them\n",
    "                confidence_scores, indices = torch.sort(probs.max(dim=1).values, descending=True)\n",
    "                sorted_preds = preds[indices]\n",
    "                sorted_targets = targets[indices]\n",
    "\n",
    "                if mode == 'easy':\n",
    "                    mask = sorted_preds == sorted_targets\n",
    "                else:  # mode == 'hard'\n",
    "                    mask = sorted_preds != sorted_targets\n",
    "\n",
    "                # Select samples and add to the indices list\n",
    "                self.indices.extend(indices[mask][:self.num_samples].tolist())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.cifar[self.indices[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "easy_train_dataset_1000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, dataloader = train_loader, transform=None, target_transform=None, mode='easy', num_samples=1000)\n",
    "\n",
    "easy_train_loader_1000 = torch.utils.data.DataLoader(dataset = easy_train_dataset_1000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "hard_train_dataset_1000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, dataloader = train_loader, transform=None, target_transform=None, mode='hard', num_samples=1000)\n",
    "\n",
    "hard_train_loader_1000 = torch.utils.data.DataLoader(dataset = hard_train_dataset_1000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "random_train_dataset_1000 = RandomCIFAR10Dataset(root = './data', transform=None, target_transform=None, num_samples=1000)\n",
    "\n",
    "random_train_loader_1000 = torch.utils.data.DataLoader(dataset = random_train_dataset_1000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\"\"\"\n",
    "\n",
    "easy_train_dataset_5000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='easy', num_samples=5000)\n",
    "\n",
    "easy_train_loader_5000 = torch.utils.data.DataLoader(dataset = easy_train_dataset_5000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "hard_train_dataset_5000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='hard', num_samples=5000)\n",
    "\n",
    "hard_train_loader_5000 = torch.utils.data.DataLoader(dataset = hard_train_dataset_5000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "random_train_dataset_5000 = RandomCIFAR10Dataset(root = './data', transform=None, target_transform=None, num_samples=5000)\n",
    "\n",
    "random_train_loader_5000 = torch.utils.data.DataLoader(dataset = random_train_dataset_5000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "                                                    \n",
    "easy_train_dataset_25000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='easy', num_samples=25000)\n",
    "\n",
    "easy_train_loader_25000 = torch.utils.data.DataLoader(dataset = easy_train_dataset_25000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "hard_train_dataset_25000 = EasyHardCIFAR10Dataset(root = './data', pretrained_model = pretrained_model, transform=None, target_transform=None, mode='hard', num_samples=25000)\n",
    "\n",
    "hard_train_loader_25000 = torch.utils.data.DataLoader(dataset = hard_train_dataset_25000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "random_train_dataset_25000 = RandomCIFAR10Dataset(root = './data', transform=None, target_transform=None, num_samples=25000)\n",
    "\n",
    "random_train_loader_25000 = torch.utils.data.DataLoader(dataset = random_train_dataset_25000,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    shuffle = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893abb26-d2bb-406a-ab58-9d68cf9c6438",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:17:50.847169Z",
     "iopub.status.busy": "2023-05-31T18:17:50.846886Z",
     "iopub.status.idle": "2023-05-31T18:17:50.876450Z",
     "shell.execute_reply": "2023-05-31T18:17:50.875885Z",
     "shell.execute_reply.started": "2023-05-31T18:17:50.847147Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model,train_loader, num_epochs, lr = .0008, weight_decay = .0008, gamma = .15, milestones = [50,65,80]):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    scheduler = MultiStepLR(optimizer, milestones=milestones, gamma= gamma)\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = cost(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "            \n",
    "def test(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(test_loader): \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            test_output = model(images)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            correct += (pred_y == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        accuracy = correct / total\n",
    "\n",
    "    print('Test Accuracy:', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def get_parameters_to_prune(model):\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    return tuple(parameters_to_prune)\n",
    "\n",
    "def sparsity_print(model):\n",
    "    prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=0)\n",
    "    zero = total = 0\n",
    "    for module, _ in get_parameters_to_prune(model):\n",
    "        zero += float(torch.sum(module.weight == 0))\n",
    "        total += float(module.weight.nelement())\n",
    "    print('Number of Zero Weights:', zero)\n",
    "    print('Total Number of Weights:', total)\n",
    "    print('Sparsity', zero/total)\n",
    "    #TODO: Implement Node Sparsity\n",
    "    return zero, total\n",
    "\n",
    "#Standard IMP with Weight Rewinding, name is a string that allows us to save models/logs appropriately, path is the location of folder we save to, start_iter should be 0 but if a experiment stops halfway through it allows us to begin there,\n",
    "#amount = % params pruned each pruning iteration, save_model downloads each model at every iter, reinit is boolean value if we want to test results on reinitialized weights\n",
    "#reinit_model is the specific model that holds the reinitialized weights\n",
    "def LotteryTicketRewinding(model, name, path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, k = 1, amount = .2, save_model = True, seed = 0, reinit = False, reinit_model = None):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    acc = []\n",
    "    reinit_acc = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    #Create Rewind Weights after training K epochs\n",
    "    train(model, train_loader,num_epochs = k)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    #Finish off the pretraining\n",
    "    train(model, train_loader,num_epochs = num_epochs - k)\n",
    "\n",
    "    #Lottery Ticket Rewinding: Prune, Rewind, Train\n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('LTR Iteration:', i+1)\n",
    "        #Prune\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "        #Train Weights\n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        \n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        if reinit:\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals,reinit_acc]))\n",
    "    \n",
    "    pass\n",
    "  \n",
    "def DataDiet(model, name, path, subset_loader, train_loader, test_loader, start_iter = 0, end_iter = 30, subset_epochs = 120, num_epochs = 60, k = 1, amount = .2, save_model = True, seed = 0, reinit = False, reinit_model = None):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    acc = []\n",
    "    reinit_acc = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    #Create Rewind Weights after training K epochs\n",
    "    train(model, train_loader,num_epochs = k)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    #Finish off the pretraining\n",
    "    train(model, train_loader,num_epochs = num_epochs - k)\n",
    "\n",
    "    #Lottery Ticket Rewinding: Prune, Rewind, Train\n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('LTR Iteration:', i+1)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "        #Train Weights\n",
    "        train(model, subset_loader,num_epochs = subset_epochs)\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        \n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "                \n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        if reinit:\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals,reinit_acc]))\n",
    "    \n",
    "    pass\n",
    "#Generate full sparsity curve with retraining for random pruning, this is not a method to compute a single, high-sparisty model with random pruning. This generates and trains models at all sparsities for comparison\n",
    "def RandomPruning(model, name, path, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs = 60, amount = .2, save_model = True, seed = 0):\n",
    "    torch.manual_seed(seed)\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    acc = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    \n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('Random Pruning Iteration:', i+1)\n",
    "        #Prune\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.RandomUnstructured,amount=amount)\n",
    "        #Train Weights\n",
    "        train(model, train_loader,num_epochs = num_epochs)\n",
    "        \n",
    "        #Log Results\n",
    "        zero, total = sparsity_print(model)\n",
    "        zeros.append(zero)\n",
    "        totals.append(total)\n",
    "        acc.append(test(model, test_loader))\n",
    "\n",
    "            \n",
    "        #Rewind Weights to save them\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "                \n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        np.save(path + name + '_log', np.array([acc,zeros,totals]))\n",
    "    \n",
    "#Generate full sparsity curve to compare with LTR\n",
    "def DistilledPruning(model, name, path, images_train, labels_train, train_loader, test_loader, start_iter = 0, end_iter = 30, num_epochs_distilled = 1000, num_epochs_real = 60, k = 0, amount = .2, save_model = True, validate = False, seed = 0, reinit = False, reinit_model = None, distilled_lr = .01):\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    accs = []\n",
    "    zeros = []\n",
    "    totals = []\n",
    "    reinit_acc = []\n",
    "    \n",
    "    model_rewind = copy.deepcopy(model).to(device)\n",
    "    torch.save(model.state_dict(), path + name + '_RewindWeights' + '_' + str(k))\n",
    "    \n",
    "    if k != 0:\n",
    "        args = argparse.Namespace(lr_net=str(distilled_lr), device='cuda', epoch_eval_train=str(k),batch_train=512,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "        model_rewind, acc_train_list, acc_test = evaluate_synset(0, model_rewind,images_train,labels_train,test_loader,args)\n",
    "        \n",
    "    \n",
    "    for i in range(start_iter,end_iter):\n",
    "        print('Distilled Pruning Iteration ', i)\n",
    "        args = argparse.Namespace(lr_net='.01', device='cuda', epoch_eval_train=str(num_epochs_distilled),batch_train=512,dataset='cifar10',dsa=True,dsa_strategy='color_crop_cutout_flip_scale_rotate',dsa_param = ParamDiffAug(), dc_aug_param=None, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)) #, zca_trans=kornia.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
    "        #MTT Training on Distilled Data\n",
    "        model, acc_train_list, acc_test = evaluate_synset(i+1, model,images_train,labels_train,test_loader,args)\n",
    "        prune.global_unstructured(get_parameters_to_prune(model),pruning_method=prune.L1Unstructured,amount=amount)\n",
    "        #Rewind Weights\n",
    "        for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "            with torch.no_grad():\n",
    "                module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                module.weight_orig.copy_(module_rewind.weight)\n",
    "    \n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), path + name + '_iter' + str(i+1))\n",
    "            \n",
    "        if validate:\n",
    "            train(model, train_loader,num_epochs = num_epochs_real)\n",
    "            accs.append(test(model, test_loader))\n",
    "            zero, total = sparsity_print(model)\n",
    "            zeros.append(zero)\n",
    "            totals.append(total)\n",
    "            #Rewind Weights\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "                    \n",
    "            np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n",
    "        \n",
    "        if reinit:\n",
    "            #Rewind Weights to Reinit Model\n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_reinit = get_parameters_to_prune(reinit_model)[idx][0]\n",
    "                    module.weight_orig.copy_(module_reinit.weight)\n",
    "                    \n",
    "            train(model, train_loader,num_epochs = num_epochs_real)\n",
    "            reinit_acc.append(test(model, test_loader))\n",
    "            \n",
    "            for idx, (module, _) in enumerate(get_parameters_to_prune(model)):\n",
    "                with torch.no_grad():\n",
    "                    module_rewind = get_parameters_to_prune(model_rewind)[idx][0]\n",
    "                    module.weight_orig.copy_(module_rewind.weight)\n",
    "            np.save(path + name + '_log', np.array([accs, zeros, totals, reinit_acc]))\n",
    "        else:\n",
    "            reinit_acc.append(0)\n",
    "            \n",
    "    if not validate:\n",
    "        train(model, train_loader,num_epochs = num_epochs_real)\n",
    "        acc = (test(model, test_loader))\n",
    "        zero, total = sparsity_print(model)\n",
    "        np.save(path + name + '_log', np.array([acc, zero, total, reinit]))\n",
    "    \n",
    "path = './model_results_cifar10/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c7b1a77-c29e-4917-9652-134f82699f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:18:19.670519Z",
     "iopub.status.busy": "2023-05-31T18:18:19.669742Z",
     "iopub.status.idle": "2023-05-31T18:19:57.756311Z",
     "shell.execute_reply": "2023-05-31T18:19:57.755582Z",
     "shell.execute_reply.started": "2023-05-31T18:18:19.670496Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(i)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mDataDiet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiet_c10_easy1000_seed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43measy_train_loader_1000\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreinit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [11], line 132\u001b[0m, in \u001b[0;36mDataDiet\u001b[0;34m(model, name, path, subset_loader, train_loader, test_loader, start_iter, end_iter, subset_epochs, num_epochs, k, amount, save_model, seed, reinit, reinit_model)\u001b[0m\n\u001b[1;32m    129\u001b[0m model_rewind \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#Finish off the pretraining\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#Lottery Ticket Rewinding: Prune, Rewind, Train\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iter,end_iter):\n",
      "Cell \u001b[0;32mIn [11], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, num_epochs, lr, weight_decay, gamma, milestones)\u001b[0m\n\u001b[1;32m      6\u001b[0m total_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):  \n\u001b[1;32m      9\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/cifar.py:115\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    111\u001b[0m img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets[index]\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/PIL/Image.py:2943\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromarray\u001b[39m(obj, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2905\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2906\u001b[0m \u001b[38;5;124;03m    Creates an image memory from an object exporting the array interface\u001b[39;00m\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;124;03m    (using the buffer protocol).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2941\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.1.6\u001b[39;00m\n\u001b[1;32m   2942\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2943\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__array_interface__\u001b[49m\n\u001b[1;32m   2944\u001b[0m     shape \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2945\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    torch.manual_seed(i)\n",
    "    model = AlexNet(3,10).to('cuda')\n",
    "    DataDiet(model, 'diet_c10_easy1000_seed' + str(i), path, easy_train_loader_1000, train_loader, test_loader, start_iter = 0, end_iter = 30, subset_epochs = 120, num_epochs = 60, k = 1, amount = .2, save_model = False, seed = i, reinit = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed1cb72-4e69-4105-8e8f-d6a9571fc36b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-31T18:00:22.869654Z",
     "iopub.status.busy": "2023-05-31T18:00:22.869396Z",
     "iopub.status.idle": "2023-05-31T18:10:19.137498Z",
     "shell.execute_reply": "2023-05-31T18:10:19.136729Z",
     "shell.execute_reply.started": "2023-05-31T18:00:22.869634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-05-31 18:00:22,872] A new study created in memory with name: no-name-122dc726-00e2-468f-abca-f4d2e31c164b\n",
      "[I 2023-05-31 18:01:55,055] Trial 0 finished with value: 0.0768 and parameters: {'learning_rate': 0.0024562846250703882}. Best is trial 0 with value: 0.0768.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-05-31 18:03:26,846] Trial 1 finished with value: 0.1542 and parameters: {'learning_rate': 0.0007407817399856925}. Best is trial 1 with value: 0.1542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-05-31 18:04:58,492] Trial 2 finished with value: 0.0948 and parameters: {'learning_rate': 0.003599480692390955}. Best is trial 1 with value: 0.1542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-05-31 18:06:29,706] Trial 3 finished with value: 0.0882 and parameters: {'learning_rate': 0.0027047571308083944}. Best is trial 1 with value: 0.1542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-05-31 18:08:01,426] Trial 4 finished with value: 0.1123 and parameters: {'learning_rate': 0.002658421638681669}. Best is trial 1 with value: 0.1542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-05-31 18:09:33,040] Trial 5 finished with value: 0.1187 and parameters: {'learning_rate': 0.003938279254994967}. Best is trial 1 with value: 0.1542.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.1187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-05-31 18:10:18,718] Trial 6 failed with parameters: {'learning_rate': 0.0032565711060557915} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_32/1021480039.py\", line 8, in objective\n",
      "    train(model, easy_train_loader_1000, 30, lr = params['learning_rate'])\n",
      "  File \"/tmp/ipykernel_32/96052406.py\", line 8, in train\n",
      "    for i, (images, labels) in enumerate(train_loader):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 681, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 721, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/tmp/ipykernel_32/2804126569.py\", line 48, in __getitem__\n",
      "    return self.cifar[self.indices[index]]\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/datasets/cifar.py\", line 118, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\", line 94, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\", line 134, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\", line 164, in to_tensor\n",
      "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
      "KeyboardInterrupt\n",
      "[W 2023-05-31 18:10:18,720] Trial 6 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test(model, test_loader)\n\u001b[1;32m     11\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler())\n\u001b[0;32m---> 12\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [9], line 8\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AlexNet(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43measy_train_loader_1000\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test(model, test_loader)\n",
      "Cell \u001b[0;32mIn [7], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, num_epochs, lr, weight_decay, gamma, milestones)\u001b[0m\n\u001b[1;32m      6\u001b[0m total_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):  \n\u001b[1;32m      9\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [6], line 48\u001b[0m, in \u001b[0;36mEasyHardCIFAR10Dataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcifar\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    163\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 164\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "              'learning_rate': trial.suggest_float('learning_rate', 8e-5, 4e-3),\n",
    "              }\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    model = AlexNet(3,10).to('cuda')\n",
    "    train(model, easy_train_loader_1000, 30, lr = params['learning_rate'])\n",
    "    return test(model, test_loader)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "model = AlexNet(3,10).to('cuda')\n",
    "for i in range(10):\n",
    "    train(model, easy_train_loader_1000, 30)\n",
    "    test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
